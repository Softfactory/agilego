# 신경망

## 신경망의 기초

딥 러닝은 신경망을 근간으로 하고 있습니다. 엄밀히 말하면 인공신경망(Artficial Neural Network) 입니다만 이글은 학문을 하려고 만든 것이 아니므로 그냥 신경망이라고 단순히 부르겠습니다.
앞으로 많은 부분들이 신경망을 이야기 하고 다루어야 하기에 기본적인 개념을 우선 정리하고 가려고 합니다.

딥 러닝은 사실 다층 구조의 신경망을 훈련하는 새로운 기술일 뿐입니다.
딥 러닝은 중첩된 유형의 신경망을 훈련시킬수 있는 여러 알고리즘을 내포하고 있습니다.

### 뉴런과 층

뉴런은 학자마다  다양한 특성을 지닌 것으로 정의하고 있어서 일관된 정의를 얻기는 힘듭니다.

> 신경망 이론은 수학적으로 그래프 이론에서 출발합니다.
> 앞으로의 용어는 수학적 용어를 해석하는 과정에서 발생한 어색함을 줄일것 입니다.
> 노드, 유닛등의 용어는 가능한 뉴런으로 통일합니다.
> 예를 들면 국내 번역물에서 일반화된 바이어스 노드를 `바이어스 뉴런`으로 표시합니다.

컴퓨터 학계에서 대략 일치하는 정의는 다음과 같습니다.

뉴런은 하나 이상의 소스(다른 뉴런이나 입력 데이터 등)로 부터 입력을 받습니다.
입력 데이터는 floating 형이나 이진형입니다.
이진형은 [0,1]을 대개 사용하지만 [1,-1]를 사용하기도 합니다.

뉴런은 가중치를 갖게 되며 입력된 값들 각각에 가중치를 곱합니다.
이렇게 곱한 값들의 합을 활성화 함수([activation function](http://www.aistudy.co.kr/neural/activation_function.htm))에 전달합니다.
활성화 함수를 거친 출력값은 수식으로 표현하면 다음과 같습니다.

$$
f(x_i,w_i) = \phi(\sum_i(w_i \cdot x_i))
$$

위 식에서 그리스 문자 파이($\phi$)는 활성화 함수를 의미합니다.
물론 어떤 신경망에서는 활성화 함수를 사용하지 않을 수도 있습니다.

신경망에 대해 [Wikipedia](https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D) 에서는 다음과 같은 특징으로 정의할 수 있다고 합니다.
* 다른 층의 뉴런들 사이의 연결 패턴
* 연결의 가중치를 갱신하는 학습 과정
* 뉴런의 가중 입력을 활성화도 출력으로 바꿔주는 활성화 함수

![인공신경망](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)

그런데 프로그래밍을 위해서 더 중요하게 알아야 할 특징이 있습니다.
* 같은 층에 있는 뉴런은 동일한 활성화 함수를 갖는다. 층이 다르면 서로 다른 활성화 함수를 갖을 수 있습니다.
* 한 층에 있는 단일 뉴런은 상위 층의 모든 뉴런과 연결된다.
* 은닉층은 없을 수도 있고 하나 이상일 수도 있다. 딥러닝을 구현하는 경우라면 2개 이상의 은닉층을 찾아보기 힘들다.
* 화살표에는 방향이 있을 수 있다.

위 그림은 전방전달신경망 (Feedforward Neural Network) 를 표시한 그림입니다.

### 뉴런의 유형

신경망 내에서 뉴런은 다양한 역할을 수행합니다. 뉴런의 역할을 기준으로 유형을 나누어 볼 수 있습니다.


#### 입력 뉴런, 출력 뉴런
거의 모든 신경망에 데이터를 입력받는 뉴런과 결과를 출력하는 뉴런이 있습니다.
대부분 이들은 역할이 나뉘어 있지만 같은 층이 입력과 출력 역할을 수행하는 경우(Hopfield Neural Network)도 있습니다.
프로그램에서 입력은 대개 배열아니 벡터로된 데이터입니다.
일반적인 경우 입력 벡터의 길이는 입력층에 있는 뉴런의 수와 같아야 합니다. 출력의 경우도 마찬가지 입니다.

#### 은닉 뉴런
다른 뉴런으로부터 데이터를 입력받거나 다른 뉴런으로만 출력을 보내는 뉴런을 은닉 뉴런이라고 합니다.
은닉 층의 수는 한때 신경망 최적화의 대상이었고 보편적 근사기(Universal Approximator)로 단일 은닉층이 학계에서 일반화되었습니다.
이유는 추가적인 은닉층 마다 학습에 더 많은 노력이 들어가기 때문이었습니다.
하지만 딥러닝은 이런 일반화를 깨뜨리게 됩니다.

#### 바이어스 뉴런
바이어스 뉴런은 패턴을 학습하기 위한 용도로 사용됩니다. 상위의 어떤 층과 연결되지 않고 단일한 값(대개는 1)을 다른 층에 전달합니다.
이때 사용되는 값을 `바이어스 활성화`라고 합니다.  

#### 컨텍스트 뉴런
컨텍스트 뉴런은 순환 신경망(Recurrent Neural Network)에 사용됩니다. 신경망의 상태를 유지하는 역할을 수행합니다.
이를 이용하면 입력값이 동일하더라도 상태에 따라서 출력값이 달라지게 됩니다.
상황에 따라 다르게 반응하는 뇌와 유사하게 동작하게 합니다. 이를 잘 활용한 예는 시계열 데이터의 분석과 기계 번역, 음성 인식 분야입니다.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Elman_srnn.png/220px-Elman_srnn.png)
#### 노드, 유닛, 합성  
많은 연구자들은 신경망 내의 모든 역할을 뉴런이 하는 것으로 생각하지는 않습니다. 그들은 이를 뉴런과 구별하기 위해 노드, 유닛, 합성이라고 부릅니다.

### 활성화 함수
활성화 함수(activation 또는 transfer function) 은 입력 데이터의 형식에 영향을 미치기 때문에 어떤 함수를 선택하느냐가 중요합니다.
활성화 함수에는 다양한 종류가 있지만 간략하게 살펴봅니다.

#### 선형(Linear) 함수
입력 뉴런에서 받은 값을 그대로 출력하는 아주 기본적인 경우를 생각해볼 수 있습니다.
수식은 다음과 같이 정의됩니다.

$$
\phi(x) = x
$$

수치형 값을 다루는 회귀 신경망(Regression Neural Network)에서 사용됩니다.
또한, 분류 신경망(Classification Neural Network)은 소프트맥스 활성화 함수를 사용합니다.

#### 계단(Step) 함수
신경망은 퍼셉트론(Perceptron)이라고 불렸는데 최초의 퍼셉트론에 계단함수가 사용되었습니다. (McCulloch & Pitts, 1943)

$$
\phi(x)=\begin{cases} 1, \quad \text{if} x \ge 0.5. \\ 0, \quad \text{otherwise}. \end{cases}
$$

임계치(Threshold) 함수로 불리기도 합니다.

#### 시그모이드(Sigmoid) 함수
전방전달 신경망에서 양의 값을 출력할 필요가 있을 때 사용될 수 있지만 이보다는 Rectified Linear Unit(ReLU) 나 Hyperbolic Tangent 가 더 유용한 선택입니다.

$$
\phi (x)=\frac { 1 }{1+ { e }^{-x  } }
$$

#### 쌍곡 탄젠트(Hyperbolic Tangent) 함수
[-1,1] 사이의 값을 출력해야 할 때 일반적으로 사용되는 함수입니다.

$$
\phi (x)=tanh(x)
$$

#### 소프트맥스(softmax)
분류 신경망에서 출력층에 사용되는 함수입니다. 특정 클래스에 속할 확률을 표시해 줍니다.
이 함수는 각 클래스에 속할 확률의 합의 1이어야 한다는 사실에서 출발합니다.

$$
\phi_i = \frac{e^{z_i}}{\sum\limits_{j \in group}e^{z_j}}
$$

위에서 i는 출력 뉴런의 순서이고, 모든 클래스에 속하는 뉴런의 순서입니다.
Z는 출력 뉴런의 배열을 표시합니다.

#### ReLU (Rectified Linear Unit)
Teh & Hinton 2000 의 연구가 있기전까지 로지스틱 시그모이드나 쌍곡탄젠트가 가장 유용한 선택이었습니다. 그러나 층이 많아지게 되면 수렴(Gradient Vanishing ) 또는 발산(Gradient Exploding)하는 문제가 생겼고, 이를 경험적으로 제거한 것이 ReLU입니다.
ReLU의 아이디어는 전력을 안정화시키는 것과 같이 정류기 역할을 하는 유닛을 신경망안에 넣는 것이었습니다.
대개 은닉층에만 사용하고 출력층에는 softmax나 선형 함수를 사용합니다.

$$
\phi (x)=max(0, x)
$$

ReLU는 성능상 많은 이점을 가지고 있습니다.
Recurrent Neural Network에서는 Gradient Vanishing 문제가 아니라 기울기가 계속 (Recurrent) 곱해지는 문제여서 주로 시그모이드가 사용됩니다.

![출처 : 위키피디아](https://upload.wikimedia.org/wikipedia/en/6/6c/Rectifier_and_softplus_functions.svg)

이론적인 유도 과정은 [위키피디아](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))를 참고하시기 바랍니다.

## GO로 Plotting

### 단순하게 GNUPlot 사용하기
GNUPlot을 이용해 다양한 그래프를 표현할 수 있습니다.
가장 단순한 방법으로는 gnuplot 패키지를 직접 호출하는 방식입니다.

```go
import "bitbucket.org/binet/go-gnuplot/pkg/gnuplot"

func plotFunction(strFunction string) {
	fname := ""
	persist := false
	debug := true

	p, err := gnuplot.NewPlotter(fname, persist, debug)
	if err != nil {
		errStr := fmt.Sprintf("** err: %v\n", err)
		panic(errStr)
	}
	defer p.Close()

	var cmd = "plot " + strFunction

	//Linear
	p.CheckedCmd(cmd)
	p.CheckedCmd("q")
	return
}
```

### GO chart 패키지로 그리기

> **TODO**

### gonum 패키지로 그리기

GNUPlot 을 이용해서 직접 그리는 것은 리소스 제약이 있을 수 있습니다.
GNUPlot이 설치되지 않은 OS에서는 설치를 권해야 합니다.
`Gonum` 프로젝트를 이용해서 Plot을 그릴 수 있습니다.

```go
func plotGonumfunc(title string, f func(float64) float64, scale ...float64) {
	// plot 구조체에 제목을 설정합니다.
	p, err := plot.New()
	if err != nil {
		panic(err)
	}
	p.Title.Text = title
	p.X.Label.Text = "X"
	p.Y.Label.Text = "Y"

	//확률분포함수를 지정하고 plot을 정의합니다.
	pdf := plotter.NewFunction(f)
	pdf.Color = color.RGBA{R: 255, A: 255}
	pdf.Dashes = []vg.Length{vg.Points(2), vg.Points(2)}
	pdf.Width = vg.Points(2)

	p.Add(pdf)
	if scale != nil && len(scale) == 4 {
		p.X.Min = scale[0]
		p.X.Max = scale[1]
		p.Y.Min = scale[2]
		p.Y.Max = scale[3]
	} else {
		p.X.Min = -1
		p.X.Max = 1
		p.Y.Min = -1
		p.Y.Max = 1
	}

	var fileName = title + ".png"
	//PNG 파일로 저장합니다.
	if err := p.Save(10*vg.Inch, 10*vg.Inch, fileName); err != nil {
		panic(err)
	}
}
```
데이터 분석을 위해서는 어떤 방법이든지 상관이 없지만 개인적으로는 `gonum/plot` 패키지를 사용할 것을 권장합니다.
단 `gonum/plot`을 사용하기 위해서는 `go get`을 통해 설치 순서가 있으니 주의하시기 바랍니다.

```shell
go get github.com/gonum/lapack
go get github.com/gonum/blas
go get github.com/gonum/internal/asm
go get github.com/gonum/matrix
go get github.com/gonum/stat
go get github.com/gonum/plot
```

### 학습을 통해 알게된 것

### 함수의 가변인자

`GO` 에서 가변인자는 아래와 같이 변수명 다음에 `... 데이터형`으로 정의할 수 있습니다.

```go
func plotGoumFunc(title string, f func(float64) float64, scale ...float64) {}
```

가변인자는 슬라이스로 인식되지만 변수를 넘겨줄 때 슬라이스 타입을 바로 사용할 수 없습니다.
테스트 코드에서 `plotGonumfunc(scale...)` 같이 `...` 을 지정해 주어야합니다.

### Delve - GO를 위한 디버거
GNUPlot 방식은 커맨드를 직접 사용하기 때문에 테스트를 실행하면 plot을 제대로 확인할 수 없습니다.
창이 바로 닫히기 때문입니다.
이를 위해 아래 부분에 Breakpoint를 걸어야 합니다.

```go
p.CheckedCmd("q")
```

이때 디버거가 필요한데 오픈소스로 [Delve](https://github.com/derekparker/delve/) 가 있습니다.
GO에서 기본제공하는 GDB는 여전히 불편하여 개발자 진영에서 그닥 환영받지 못했습니다.
Delve는 아직 Preview 단계이지만 많은 개발자에게서 지지를 얻고 있습니다.
Delve를 직접 사용할 일은 없습니다만 Atom 에디터에서 제공하는 `go-debug`는 숙달을 해야 합니다.
상용에디터를 가지고 있을 여유가 있다면 이 부분은 해당사항이 없겠죠.
